{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar\n",
    "#import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import os\n",
    "#from bs4 import BeautifulSoup\n",
    "#import re\n",
    "#from collections import OrderedDict\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "#import getpass\n",
    "import pyshorteners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realizar_login(url, login1, password1, orgao1):\n",
    "    \"\"\"\n",
    "    Função para realizar login no sistema SEI.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL do sistema SEI.\n",
    "        login1 (str): Nome de usuário para login.\n",
    "        password1 (str): Senha para login.\n",
    "        orgao1 (str): Nome do órgão para acesso.\n",
    "\n",
    "    Returns:\n",
    "        webdriver: Instância do WebDriver com o usuário autenticado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Inicializa o navegador\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.implicitly_wait(0.5)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Localiza os elementos de login\n",
    "        login = driver.find_element(By.XPATH, '//*[@id=\"txtUsuario\"]')\n",
    "        password = driver.find_element(By.XPATH, '//*[@id=\"pwdSenha\"]')\n",
    "        orgao = driver.find_element(By.XPATH, '//*[@id=\"selOrgao\"]')\n",
    "        submit_button = driver.find_element(By.XPATH, '//*[@id=\"Acessar\"]')\n",
    "\n",
    "        # Preenche as credenciais\n",
    "        login.send_keys(login1)\n",
    "        password.send_keys(password1)\n",
    "        orgao.send_keys(orgao1)\n",
    "\n",
    "        # Realiza o login\n",
    "        submit_button.click()\n",
    "        time.sleep(3)  # Aguarda carregamento da página após o login\n",
    "\n",
    "        print(\"Login realizado com sucesso!\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao realizar o login: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "url = 'https://sei.economia.gov.br/'\n",
    "login1 = 'daiana.sales@gestao.gov.br'\n",
    "password1 = \"D!vertidamente081099\"\n",
    "orgao1 = \"MGI\"\n",
    "\n",
    "# Realiza login\n",
    "driver = realizar_login(url, login1, password1, orgao1)\n",
    "\n",
    "# Se o login foi bem-sucedido, realiza a busca\n",
    "if driver:\n",
    "    buscar_arquivos(driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "searching = driver.find_element(By.XPATH, '//*[@id=\"infraMenu\"]/li[14]/a/span')\n",
    "searching.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#selecionar os pesquisar em documentos\n",
    "docum_pesq = driver.find_element(By.XPATH, '//*[@id=\"divOptDocumentos\"]/div')\n",
    "docum_pesq.click()\n",
    "time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#colocar como tramitação dentro do orgão\n",
    "chktram = driver.find_element(By.XPATH, '//*[@id=\"divSinTramitacao\"]/div')\n",
    "chktram.click()\n",
    "time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Especifica os termos de pesquisa\n",
    "espec_pesq = driver.find_element(By.XPATH, '//*[@id=\"q\"]')\n",
    "espec_pesq.send_keys('indicação ou retificação ou nomear ou nomeação ou ratificar ou retificar ou representante ou exoneração')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizar pesquisa por tipo de processo\n",
    "tipo_process = driver.find_element(By.XPATH, '//*[@id=\"selTipoProcedimentoPesquisa\"]')\n",
    "tipo_process.send_keys(\"Gestão Administrativa: Conselhos, Comissões, Comitês, Grupos de Trabalho e Juntas\")\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realiza a pesquisa\n",
    "b_pesq = driver.find_element(By.XPATH, '//*[@id=\"sbmPesquisar\"]')\n",
    "b_pesq.click()\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal para busca\n",
    "def buscar_arquivos(driver):\n",
    "    \"\"\"\n",
    "    Realiza a busca de arquivos no sistema SEI após login.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): Instância do WebDriver autenticada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "                        \n",
    "        # Acessa a área de busca\n",
    "        searching = driver.find_element(By.XPATH, '//*[@id=\"infraMenu\"]/li[14]/a/span')\n",
    "        searching.click()\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        #selecionar os pesquisar em documentos\n",
    "        docum_pesq = driver.find_element(By.XPATH, '//*[@id=\"divOptDocumentos\"]/div')\n",
    "        docum_pesq.click()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # #Selecionar pesquisar em processos\n",
    "        # process = driver.find_element(By.XPATH, '//*[@id=\"divOptProcessos\"]/div')\n",
    "        # process.click()\n",
    "        # time.sleep(0.2)\n",
    "        \n",
    "        # #fazer busca através dos documentos, clicar no botão considerar documentos\n",
    "        # cons_doc = driver.find_element(By.XPATH, '//*[@id=\"divSinConsiderarDocumentos\"]/div')\n",
    "        # cons_doc.click()\n",
    "        # time.sleep(0.5)\n",
    "        \n",
    "        #colocar como tramitação dentro do orgão\n",
    "        chktram = driver.find_element(By.XPATH, '//*[id=\"chkSinTramitacao\"]/div')\n",
    "        chktram.click()\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        # Restringe busca ao órgão específico\n",
    "        sel_orgao = driver.find_element(By.XPATH, '//*[@id=\"divSinRestringirOrgao\"]/div')\n",
    "        sel_orgao.click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # Especifica os termos de pesquisa\n",
    "        espec_pesq = driver.find_element(By.XPATH, '//*[@id=\"q\"]')\n",
    "        espec_pesq.send_keys('indicação ou retificação ou nomear ou nomeação ou ratificar ou retificar ou representante ou exoneração')\n",
    "        \n",
    "        # Realizar pesquisa por tipo de processo\n",
    "        tipo_process = driver.find_element(By.XPATH, '//*[@id=\"selTipoProcedimentoPesquisa\"]')\n",
    "        tipo_process.send_keys(\"Gestão Administrativa: Conselhos, Comissões, Comitês, Grupos de Trabalho e Juntas\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # # colocar datas\n",
    "        data_inicio = driver.find_element('xpath', '//*[@id=\"txtDataInicio\"]')\n",
    "        data_inicio.send_keys(\"01/09/2024\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Realiza a pesquisa\n",
    "        b_pesq = driver.find_element(By.XPATH, '//*[@id=\"sbmPesquisar\"]')\n",
    "        b_pesq.click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        print(\"Busca realizada com sucesso.\\nRestringindo em tipo de processo \\n Gestão Administrativa: Conselhos, Comissões, Comitês, Grupos de Trabalho e Juntas \\n e dentro do MGI.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante a busca: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extrair_dados(driver):     \n",
    "    \"\"\"\n",
    "    Função para realizar web scraping no SEI e retornar os dados em um DataFrame.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): Instância do Selenium WebDriver.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo os dados extraídos.\n",
    "    \"\"\"\n",
    "    def remove_items(lista, item): \n",
    "        \"\"\"Remove todos os itens iguais a `item` de uma lista.\"\"\"\n",
    "        return [i for i in lista if i != item]\n",
    "\n",
    "    # Extraindo os elementos da pesquisa\n",
    "    tree_elements = driver.find_elements(\"xpath\", '//*[@class=\"pesquisaTituloEsquerda\"]/a')\n",
    "    # //*[@id=\"conteudo\"]/table/tbody/tr[1]/td[1]/span/text()\n",
    "    \n",
    "    list_tree = [element.text for element in tree_elements]\n",
    "    trees = remove_items(list_tree, '')  # Remover elementos vazios\n",
    "\n",
    "    # abts = driver.find_elements(\"xpath\", '//*[@class=\"pesquisaSnippet\"]')\n",
    "    # list_abts = [element.text for element in abts]\n",
    "\n",
    "    unidades = driver.find_elements(\"xpath\", '//*[@class=\"pesquisaMetatag\"]')\n",
    "    list_uni = [element.text.split(':') for element in unidades]\n",
    "    info = [sublist_uni[1] for sublist_uni in list_uni if len(sublist_uni) > 1]  # Removendo listas vazias\n",
    "\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"conteudo\"]/table/tbody/tr')\n",
    "    links = []\n",
    "    files_name = []\n",
    "    for i in range(1, len(rows)+1, 3):\n",
    "        try:\n",
    "            a = driver.find_element(By.XPATH, f'//*[@id=\"conteudo\"]/table/tbody/tr[{i}]/td[2]/a')\n",
    "            time.sleep(0.5)\n",
    "            link = a.get_attribute('href')\n",
    "            file_name = a.text\n",
    "            links.append(link)\n",
    "            files_name.append(file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a linha {i}: {e}\")\n",
    "\n",
    "    # Inicializando o encurtador de links\n",
    "    shortener = pyshorteners.Shortener(api_key='your_api_key', provider='isgd')\n",
    "    links_curtos = []\n",
    "    erros = []\n",
    "    \n",
    "    for link in links:\n",
    "        try:\n",
    "            # Separando a parte fixa e o hash do link\n",
    "            base_url, hash_value = link.split('infra_hash=')\n",
    "            \n",
    "            # Verificando o tamanho da parte fixa do link antes de encurtar\n",
    "            if len(base_url) > 1000:  # Ajuste o limite conforme necessário\n",
    "                raise ValueError(f\"Link base muito longo: {base_url}\")\n",
    "            \n",
    "            # Encurtando a parte fixa do link\n",
    "            link_curto = shortener.tinyurl.short(base_url)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Recriando o link com o hash\n",
    "            link_com_hash = link_curto + 'infra_hash=' + hash_value\n",
    "            \n",
    "            links_curtos.append(link_com_hash)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            erros.append((link, str(e)))\n",
    "            links_curtos.append(None)\n",
    "    \n",
    "    # Encontrando o comprimento mínimo entre as listas\n",
    "    min_length = min(len(trees[::2]), len(trees[1::2]), len(list_abts), len(info[::3]), len(info[1::3]), len(info[2::3]), len(links_curtos))\n",
    "    \n",
    "    # Cortando todas as listas para o comprimento mínimo\n",
    "    dados = {\n",
    "        \"Número do Processo\": trees[::2][:min_length],\n",
    "        \"Documento\": trees[1::2][:min_length],\n",
    "        \"Resumo\": list_abts[:min_length],\n",
    "        \"Unidade\": info[::3][:min_length],\n",
    "        \"Usuário\": info[1::3][:min_length],\n",
    "        \"Data de Inclusão\": info[2::3][:min_length],\n",
    "        \"Links\": links_curtos[:min_length]\n",
    "    }\n",
    "        \n",
    "    df = pd.DataFrame(dados)\n",
    "    \n",
    "    original_window = driver.current_window_handle \n",
    "    \n",
    "    for index, (link, name) in enumerate(zip(links, files_name)):\n",
    "        driver.switch_to.new_window('tab')\n",
    "        driver.get(link)\n",
    "        with open(f'./colegiado_{name}.html', 'w', encoding='utf-8', errors='ignore') as file:\n",
    "            file.write(driver.page_source)        \n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "    \n",
    "    return df, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col = extrair_dados1(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extrair_dados1(driver):     \n",
    "    \"\"\"\n",
    "    Função para realizar web scraping no SEI e retornar os dados em um DataFrame.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): Instância do Selenium WebDriver.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo os dados extraídos.\n",
    "        \n",
    "    \"\"\"\n",
    "    def remove_items(lista, item): \n",
    "        \"\"\"Remove todos os itens iguais a `item` de uma lista.\"\"\" \n",
    "        return [i for i in lista if i != item]\n",
    "   \n",
    "    # Extraindo os elementos da pesquisa\n",
    "    tree_elements = driver.find_elements(By.XPATH, '//*[@class=\"pesquisaTituloEsquerda\"]/a')\n",
    "    list_tree = [element.text for element in tree_elements]\n",
    "    trees = remove_items(list_tree, '')  # Remover elementos vazios\n",
    "\n",
    "    abts = driver.find_elements(By.XPATH, '//*[@class=\"pesquisaSnippet\"]')\n",
    "    list_abts = [element.text for element in abts]\n",
    "    \n",
    "    # Adicionando a verificação de tamanho do abstract\n",
    "    max_length = 500  # Defina o tamanho máximo permitido\n",
    "    list_abts = [abt for abt in list_abts if len(abt) <= max_length]\n",
    "\n",
    "    unidades = driver.find_elements(By.XPATH, '//*[@class=\"pesquisaMetatag\"]')\n",
    "    list_uni = [element.text.split(':') for element in unidades]\n",
    "    info = [sublist_uni[1] for sublist_uni in list_uni if len(sublist_uni) > 1]  # Removendo listas vazias\n",
    "\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"conteudo\"]/table/tbody/tr')\n",
    "    links = []\n",
    "    files_name = []\n",
    "    for i in range(1, len(rows)+1, 3):\n",
    "        try:\n",
    "            a = driver.find_element(By.XPATH, f'//*[@id=\"conteudo\"]/table/tbody/tr[{i}]/td[2]/a')\n",
    "            time.sleep(0.5)\n",
    "            link = a.get_attribute('href')\n",
    "            file_name=a.text\n",
    "            links.append(link)\n",
    "            files_name.append(file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a linha {i}: {e}\")\n",
    "\n",
    "    # print(files_name)\n",
    "\n",
    "    # Inicializando o encurtador de links\n",
    "    shortener = pyshorteners.Shortener(api_key='your_api_key', provider='isgd')\n",
    "    links_curtos = []\n",
    "    erros = []\n",
    "    \n",
    "    for link in links:\n",
    "        try:\n",
    "            # Separando a parte fixa e o hash do link\n",
    "            base_url, hash_value = link.split('infra_hash=')\n",
    "            \n",
    "            # Verificando o tamanho da parte fixa do link antes de encurtar\n",
    "            if len(base_url) > 1000:  # Ajuste o limite conforme necessário\n",
    "                raise ValueError(f\"Link base muito longo: {base_url}\")\n",
    "            \n",
    "            # Encurtando a parte fixa do link\n",
    "            link_curto = shortener.tinyurl.short(base_url)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Recriando o link com o hash\n",
    "            link_com_hash = link_curto + 'infra_hash=' + hash_value\n",
    "            \n",
    "            links_curtos.append(link_com_hash)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            erros.append((link, str(e)))\n",
    "            links_curtos.append(None)\n",
    "            \n",
    "    dados = {\n",
    "        \"Número do Processo\": trees[::2],\n",
    "        \"Documento\": trees[1::2],\n",
    "        \"Resumo\": list_abts,\n",
    "        \"Unidade\": info[::3],\n",
    "        \"Usuário\": info[1::3],\n",
    "        \"Data de Inclusão\": info[2::3],\n",
    "        \"Links\": links_curtos\n",
    "    }\n",
    "        \n",
    "    df = pd.DataFrame(dados)\n",
    "    \n",
    "    original_window = driver.current_window_handle \n",
    "    \n",
    "    for index, (link, name) in enumerate(zip(links, files_name)):\n",
    "        driver.switch_to.new_window('tab')\n",
    "        driver.get(link)\n",
    "        with open(f'./colegiados_{name}.html', 'w', encoding='utf-8', errors='ignore') as file:\n",
    "            file.write(driver.page_source)        \n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col1 = extrair_dados1(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados(driver):     \n",
    "    \"\"\"\n",
    "    Função para realizar web scraping no SEI e retornar os dados em um DataFrame.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): Instância do Selenium WebDriver.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo os dados extraídos.\n",
    "    \"\"\"\n",
    "    def remove_items(lista, item): \n",
    "        \"\"\"Remove todos os itens iguais a `item` de uma lista.\"\"\" \n",
    "        return [i for i in lista if i != item]\n",
    "\n",
    "    # Extraindo os elementos da pesquisa\n",
    "    tree_elements = driver.find_elements(\"xpath\", '//*[@class=\"protocoloNormal\"]/a')\n",
    "    list_tree = [element.text for element in tree_elements]\n",
    "    trees = remove_items(list_tree, '')  # Remover elementos vazios\n",
    "\n",
    "    unidades = driver.find_elements(\"xpath\", '//*[@class=\"pesquisaMetatag\"]')\n",
    "    list_uni = [element.text.split(':') for element in unidades]\n",
    "    info = [sublist_uni[1] for sublist_uni in list_uni if len(sublist_uni) > 1]  # Removendo listas vazias\n",
    "\n",
    "    rows = driver.find_elements(\"xpath\", '//*[@id=\"conteudo\"]/table/tbody/tr')\n",
    "    links = []\n",
    "    \n",
    "    for i in range(1, len(rows), 3):\n",
    "        try:\n",
    "            a = driver.find_element(\"xpath\", f'//*[@id=\"conteudo\"]/table/tbody/tr[{i}]/td[1]/a[2]')\n",
    "            time.sleep(0.5)\n",
    "            link = a.get_attribute('href')\n",
    "            # print(link)\n",
    "            time.sleep(0.5)\n",
    "            links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a linha {i}: {e}\")\n",
    "\n",
    "    # Inicializando o encurtador de links\n",
    "    shortener = pyshorteners.Shortener(api_key='your_api_key', provider='isgd')\n",
    "    links_curtos = []\n",
    "    erros = []\n",
    "    \n",
    "    for link in links:\n",
    "        try:\n",
    "            # Separando a parte fixa e o hash do link\n",
    "            base_url, hash_value = link.split('infra_hash=')\n",
    "            \n",
    "            # Verificando o tamanho da parte fixa do link antes de encurtar\n",
    "            if len(base_url) > 1000:  # Ajuste o limite conforme necessário\n",
    "                raise ValueError(f\"Link base muito longo: {base_url}\")\n",
    "            \n",
    "            # Encurtando a parte fixa do link\n",
    "            link_curto = shortener.tinyurl.short(base_url)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Recriando o link com o hash\n",
    "            link_com_hash = link_curto + 'infra_hash=' + hash_value\n",
    "            \n",
    "            links_curtos.append(link_com_hash)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            erros.append((link, str(e)))\n",
    "            links_curtos.append(None)\n",
    "\n",
    "    # Organizando os dados para o DataFrame\n",
    "    dados = {\n",
    "        \"Número do Processo\": trees[::2],\n",
    "        \"Documento\": trees[1::2],\n",
    "        # \"Resumo\": list_abts,  # Caso esteja disponível\n",
    "        \"Unidade\": info[::3],\n",
    "        \"Usuário\": info[1::3],\n",
    "        \"Data de Inclusão\": info[2::3],\n",
    "        \"Links\": links_curtos\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(dados)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col = extrair_dados(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//*[@id=\"conteudo\"]/table/tbody/tr[1]/td[1]/a[2]\n",
    "//*[@id=\"conteudo\"]/table/tbody/tr[4]/td[1]/a[2]\n",
    "//*[@id=\"conteudo\"]/table/tbody/tr[7]/td[1]/a[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//*[@id=\"conteudo\"]/div[2]/div[3]/a\n",
    "//*[@id=\"conteudo\"]/div[2]/div[2]/a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next_page = driver.find_element(\"xpath\", '//*[@id=\"conteudo\"]/div[2]/div[3]/a')\n",
    "print(next_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proxima_href = next_page.get_attribute('href')\n",
    "print(proxima_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next_page.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dados_consolidados = pd.DataFrame()  # DataFrame vazio para acumular os dados\n",
    "\n",
    "# Extrair os dados da página atual e adicionar ao DataFrame consolidado\n",
    "df_pagina = extrair_dados(driver)\n",
    "dados_consolidados = pd.concat([dados_consolidados, df_pagina], ignore_index=True)\n",
    "# Procurar o botão \"Próxima\"\n",
    "next_page = driver.find_element(\"xpath\", '//*[@id=\"conteudo\"]/div[2]/div[3]/a')\n",
    "proxima_href = next_page.get_attribute('href')\n",
    "print(proxima_href)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navegar_paginas(driver):\n",
    "    \"\"\"\n",
    "    Loop para navegar por todas as páginas até que não haja mais um botão 'Próxima'.\n",
    "    Retorna um DataFrame consolidado com os dados de todas as páginas.\n",
    "    \"\"\"\n",
    "    dados_consolidados = pd.DataFrame()  # DataFrame vazio para acumular os dados\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extrair os dados da página atual e adicionar ao DataFrame consolidado\n",
    "            df_pagina = extrair_dados(driver)\n",
    "            dados_consolidados = pd.concat([dados_consolidados, df_pagina], ignore_index=True)\n",
    "\n",
    "            # Procurar o botão \"Próxima\"\n",
    "            next_page = driver.find_element(\"xpath\", '//*[@id=\"conteudo\"]/div[2]/div[3]/a')\n",
    "\n",
    "            # Verificar se o botão \"Próxima\" tem o atributo 'href'\n",
    "            proxima_href = next_page.get_attribute('href')\n",
    "            print(proxima_href)\n",
    "            time.sleep(3)\n",
    "            if not proxima_href:\n",
    "                print(\"Não há mais páginas. Encerrando navegação.\")\n",
    "                break  # Sai do loop se não houver link para a próxima página\n",
    "\n",
    "            # Clicar no botão \"Próxima\"\n",
    "            next_page.click()\n",
    "            time.sleep(10)  # Aguarda o carregamento da próxima página\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(\"Botão 'Próxima' não encontrado. Encerrando navegação.\")\n",
    "            break  # Sai do loop se o botão \"Próxima\" não existir\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro inesperado: {e}\")\n",
    "            break  # Sai do loop em caso de erro inesperado\n",
    "\n",
    "    # Fechar o navegador após o término\n",
    "    # driver.close()\n",
    "    # driver.quit()\n",
    "\n",
    "    return dados_consolidados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = navegar_paginas(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realizar_login(url, login1, password1, orgao1):\n",
    "    \"\"\"\n",
    "    Função para realizar login no sistema SEI.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL do sistema SEI.\n",
    "        login1 (str): Nome de usuário para login.\n",
    "        password1 (str): Senha para login.\n",
    "        orgao1 (str): Nome do órgão para acesso.\n",
    "\n",
    "    Returns:\n",
    "        webdriver: Instância do WebDriver com o usuário autenticado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Inicializa o navegador\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.implicitly_wait(0.5)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Localiza os elementos de login\n",
    "        login = driver.find_element(By.XPATH, '//*[@id=\"txtUsuario\"]')\n",
    "        password = driver.find_element(By.XPATH, '//*[@id=\"pwdSenha\"]')\n",
    "        orgao = driver.find_element(By.XPATH, '//*[@id=\"selOrgao\"]')\n",
    "        submit_button = driver.find_element(By.XPATH, '//*[@id=\"Acessar\"]')\n",
    "\n",
    "        # Preenche as credenciais\n",
    "        login.send_keys(login1)\n",
    "        password.send_keys(password1)\n",
    "        orgao.send_keys(orgao1)\n",
    "\n",
    "        # Realiza o login\n",
    "        submit_button.click()\n",
    "        time.sleep(3)  # Aguarda carregamento da página após o login\n",
    "\n",
    "        print(\"Login realizado com sucesso!\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao realizar o login: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "url = 'https://sei.economia.gov.br/'\n",
    "login1 = 'daiana.sales@gestao.gov.br'\n",
    "password1 = \"D!vertidamente081099\"\n",
    "orgao1 = \"MGI\"\n",
    "\n",
    "# Realiza login\n",
    "driver = realizar_login(url, login1, password1, orgao1)\n",
    "\n",
    "# Se o login foi bem-sucedido, realiza a busca\n",
    "if driver:\n",
    "    buscar_arquivos(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "def gerar_excel(df):\n",
    "    output = BytesIO()\n",
    "    with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=\"Doc_SEI\")\n",
    "    processed_data = output.getvalue()\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = gerar_excel(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(excel_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspar_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
